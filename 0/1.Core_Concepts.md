# **Core Concepts**

## *Testing*

Testing is a largely intellectual activity that requires careful planning, deductive skills and a capacity for thinking about how things fit together in systems. Testing always involves a ***test object***, which is what you are testing. The testing process usually creates numerous byproducts, usually called "***work products***" or "***artifacts***". A typical test aims to discover defects before they become a problem, whether these defects are system failures of some kind or defects according to a requirement for the test object (requirements can be contractual, regulatory, technical and based on user expectations). Therefore, it is also important to evaluate the suitability of the test's own work products for these purposes. For example, the test should be designed in such a way that it achieves satisfying coverage of the test object's functionalities based on estimates of cost-to-benefit ratio in minimizing risk.
Whatever the test finds is valuable information that should be forwarded to those it might concern so they can make informed decisions. With that, the Tester participates in ***Quality Control***, which justifies confidence in the quality of the final product.

## *Quality Control (QC)*

Quality Control assumes there are defects and describes activities which assist in finding and correcting them. Testing is a major form or part of Quality Control. QC is related to ***Quality Assurance***, but hardly more than anything else is.

## *Quality Assurance (QA)*

Quality Assurance assumes that following a good process leads to a good product. You can understand it as a matter of following best practices, which means it applies to everything for which there is a best practice, including monitoring performance. Having QC is a matter of QA because while good processes lead to good products, things can and will slip through the cracks.

## *Software Development LifeCycle (SDLC)*

A Software Development LifeCycle (model) is an abstract model of how software gets developed. A SDLC shows what should happen and in what order (and why). There are some general patterns for this that implement testing in different ways, but in general the *[Test LifeCycle](/0/4.Test_LifeCycle.md)* will be the same, even if it is spaced out or automated. Regardless of the SDLC, it is a good idea to:
- ensure that each development activity has a corresponding Test LifeCycle for QC.
- Start these activities at the same time to allow for early testing.
- Get testers involved in 
Different kinds of tests are relevant at different times for different SDLCs.

## *Root Causes*

A root cause is the ultimate reason for one or more problems. Root causes are sought in root cause analysis, which usually follows the discovery of a problem. Once found, the root cause should be fixed or removed.

## *Failures*

A failure occurs when a system either fails to do what it should do or does something it should not do. The root cause may or may not be external to the SDLC.

## *Defects*

Defects are the features of a product which are in some way "faulty" or "buggy", that is, the parts that produce failures. For example, it might be a mismatch between documentation and product, or a line of code which is written incorrectly.


## *Errors*

Humans make mistakes, and in this context they are known as errors. These errors will often produce defects.

## *Verification*

Verification is one of the two main forms of comparing a test object to requirements. Verification checks if the test object meets requirements specified by the team developing it, or internal requirements.

## *Validation*

Validation is the other main form of comparing a test object to test requirements. Validation checks if the test object meets the requirements from users or others whom it may concern, or external requirements.

## *Tools*

Various tools are useful for many different purposes throughout the testing process. They come in about as many different forms, but are generally *supposed* to make your job easier in some way. Some tools make it easier to communicate. Many ***automate*** some part of the testing process, while some support the tester with, for example, a template. Some tools use templates to make your work **scale**, letting you accomplish more by doing less. Remember that even a spreadsheet is a tool! While tools are useful they are not always worth using. They might not be compatible with each other, be too foreign to your team, or not as simple as they appear. Keep this in mind when considering tools, but remember that whatever you are looking for, chances are there is a tool out there that does it for you!

## *Automation*

Automation allows processes to run without supervision and/or human input. Once something is automated it can save a lot of work and time, especially if it is a frequent or regular activity. It also removes human error from the testing process once it is set up properly. With such benefits it is easy to overestimate automation, and worth remembering that automation requires maintenance if relevant factors change (it might get deprecated,your requirements might change, etc.), and that it offers no guarantee that it *was* set up properly. This means that human errors during implementation or automation tool design can still cause defects, so it should be remembered that an automated test can also get things wrong. If it can be done with code, it can be automated.

## *Maintenance tests*

As code changes throughout development there are two kinds of testing that may need to be done repeatedly. These are Confirmation tests and Regression tests. These often go hand in hand as an important but somewhat unplannable form of testing. Its scope depends on the risks involved, the size of the system and the size of the change. It usually happens when an application is being retired (which may also require testing of restoration and retrieval procedures), a significant upgrade or migration, or as part of ordinary development.

### *Confirmation tests*

Confirmation testing checks if a defect has been fixed. If confirmation testing fails, it will need to be repeated following the next attempted fix. To check this, confirmation testing should check all known failures caused by the defect, and may also involve the creation of new tests to cover changes introduced to fix the defect.

### *Regression tests*

Regression testing checks for regressions, cases where formerly functional solutions no longer work because of a more recent change. Regressions could arise at any distance from the change, and at many different scales. As a product grows during its development, the scale of regression testing can easily grow, but there is usually a logical throughline from the change to the regression, therefore regression testing can be targeted to save resources by conducting an impact analysis before conducting manual regression tests. It is a good practice to include automated regression tests in ***DevOps*** integration pipelines.

## *DevOps*

DevOps is a fairly new way to organize the SDLC by making development (including testing) and operations (such as deployment and maintenance) work closely together to continuously deliver a product as it is being improved upon. DevOps-methods seek to automate as much as possible and to arrange things so that everything can happen faster, from defect-detection to deployment and monitoring.

## *Shift-Left*

Shift-Left is an approach to development and testing which moves everything as far to the left as possible. 'Left', because most people read from left to right, and that includes reading models, such as LifeCycle models. Hence, Shift-Left means starting every part of the LifeCycle, including testing, as soon as possible. ***DevOps*** is aligned with Shift-Left, but beyond that, it can mean writing the tests before the code (thereby requiring a tester's eye on specifications), checking the code before running it, and trying to test quality at as low a level as possible.

## *User Story*

User Stories express a wish for a feature in the developing product. The most common format for user stories is "As a [role], I want [feature], so I can [do my job better]", followed by ***Acceptance Criteria***. (They also "include" a conversation about how the software will be used, but this can be done verbally) They should be Independent, Negotiable, Valuable, Estimable, Small and Testable (INVEST *sigh*). It is a good idea to write important user stories together. Involving other *[concerned parties](/1/1/4.Concerned_Parties.md)* in the process can avoid blunders and helps ensure you have a shared vision.

### *Acceptance Criteria*

When the acceptance criteria are satisfied, the *[concerned parties](/1/1/4.Concerned_Parties.md)* are happy with the solution to the user story. These define the scope of the user story, outlining software behavior in both positive and negative scenarios, and serve as the foundation for a [test approach](/1/1/6.Test_Approach.md). The criteria can be written in any way, but are usually formatted as either a description of scenarios (the Given/When/Then format) or as rules that should be verified.

## *Configuration Management*

Configuration Management (CM) is a matter of discipline in managing testware produced during the *[Test LifeCycle](/0/4.Test_LifeCycle.md)*, ***artifacts*** of the TLC. In principle, proper CM means that every artifact has a unique ID and is subject to rigorous ***version control***, with a clear map of how it relates to other artifacts. Finally, it means that whenever an artifact is referenced, it is done unambiguously (ID helps in this respect). Once an artifact is approved for testing in CM it becomes a baseline, requiring a formal change process for any further changes.

### *Version control*

Version control means that you have a system keeping track of different versions of an item and how one is different from the next. In this context, differences can extend to include which other artifacts it relates to or how.


*[Continue here](/0/2.Principles_of_Testing.md)*