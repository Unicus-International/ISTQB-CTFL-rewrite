# *Black-box testing*

Black-box test techniques treat the test object as a black-box. A black-box is something you know does something, but which you can't look inside to check how it does what it does. Hence, these are also known as specification-based techniques, as they check the black-box' behavior and compare it to specifications. Since they check behavior they are naturally connected to dynamic testing. Here are four kinds of black-box test techniques: ***Equivalence Partitioning***, ***Boundary Value Analysis***, ***Decision Table Testing*** and ***State Transition Testing***.

## *Equivalence Partitioning*

Equivalence Partitioning (EP) organizes data in partitions where any two values in the same partition should be treated the same by the black-box; They are *equivalent* as far as the box is concerned. When they get treated the same, one test per partition is sufficient, though EP assumes that each value in one partition is equivalent, and does not verify this.

You can do EP on *any* data related to the test object, but partitions must not overlap (if they somehow do, the overlap cannot be equivalent with *both* overlapping partitions *at once*) and they must have *at least one value*. Partitions are either valid or invalid depending on their values, but the exact definitions of valid vary.

Partitioning should be done with care, since test objects are often complicated (and with black-boxes you don't know for sure). Many test objects will have multiple sets of partitions that you will need to cover. The simplest method for handling several partition-sets is Each Choice coverage. It requires every partition in each set to be exercised at least once. As you might guess, 100% coverage in EP is when you have exercised all known partitions at least once, as in Each Choice coverage. You calculate coverage like this:

```python
percent_coverage = exercised_partitions / total_partitions * 100
```

## *Boundary Value Analysis*

Boundary Value Analysis (BVA) builds on EPs by exercising the boundaries of the partitions. Hence, it only applies to ordered partitions (think: numbers). BVAs anticipate developer errors, which are more likely at partition boundaries. Perhaps the developer forgot to set the limit or set it incorrectly? If so, a BVA can catch it. Different kinds of BVAs use different numbers of items for each examined boundary. Coverage is calculated like this:

```python
percent_coverage = exercised_items / total_items * 100
```

A 2-value BVA needs to cover two items for each boundary value in the examined partition. Namely, the boundary value and the boundary value of the bordering partition.

A 3-value BVA needs to cover three items for each boundary value. It checks the boundary value itself and both the boundary value of the bordering partition and the value immediately before or after itself in its own partition. 3-value BVA is more rigorous because it can catch logical errors where the partition's boundary is set incorrectly. For example, if what was supposed to be x â‰¤ 10 was implemented as x = 10, a 2-value BVA might only check x values 10 and 11, and not discover that there are issues when x < 10, as it is when x = 9.

## *Decision Table Testing*

Decision tables record complex logic, specifying how different combinations of conditions produce different behavior from the test object. You start with the various conditions and the resulting test object actions. Each condition that matters gets a line along your chosen axis (columns make intuitive sense but the syllabus suggests rows), as does the action(s if there are more than one). Each line along the other axis (rows make intuitive sense but the syllabus suggests columns) defines a unique combination of conditions and the associated feasible and/or relevant actions. (Any combination that can occur in the real world is feasible.) A full decision table has a line for every combination, but it can be simplified by excluding combinations that are not feasible. You can also merge lines by using EP to simplify the table. Decision table coverage is calculated like this:

```python
percent_coverage = exercised_combinations / total_feasible_combinations * 100
```

Decision tables let you identify combinations of conditions systematically. This lets you discover gaps and contradicitons that could have gone unnoticed. Note that with more conditions a decision table grows exponentially. When that becomes a problem, consider minimizing it. For example, you can take a *[risk-based approach](/1/1/1.Risk_Register.md#risk-based-test-approach)* by excluding combinations with a low risk-level.

There are two main patterns for Decision Table design: ***Limited-entry*** and ***Extended-entry***. In both cases the symbols might vary, but these are typical.

### *Limited-entry*

A limited-entry decision table limits the entries for combinations to Boolean values (either true or false), represented by 'T' (True) or 'F' (False). '-' represents irrelevant conditions, and 'N/A' (Not Applicable) represents infeasible ones. Actions sometimes get displayed as 'X' if they should occur, and get no symbol if they should not. Limited-entry tables are usually better at a small scale due to their compactness.

### *Extended-entry*

An extended-entry decision table differs in that it allows for more complex symbols in the combinations. For example, a condition in a combination might be an EP or a discrete (that is, specific) value. Extended-entry tables allow a single action field on your chosen axis to display all resulting actions. These two factors mean that that they scale better with test object complexity.

## *State Transition Testing*

State transition testing uses a model of the test object's states and possible transitions to either visit the states the test object can reach, or exercise the transitions of the test object. A transition is triggered by an event, and can be guarded by a condition that prevents the transition. Transitions can also result in additional behavior from the test object.

The models will usually either be a diagram or a table. In a state transition diagram you will usually see a sequence of states connected by arrows that represent the transitions. The common syntax for events, guards and actions is "event [guard condition] / action".
In tables, rows usually represent different states, columns represent the state transitions. In tables, it is important that the transitions tell you what state they transition to, as you can't use arrows for that in tables.

A typical state transition test case is written as a sequence of events for the tester to perform, leading to a sequence of transitions and states. Hence, State transition test cases usually cover several transition.

There are three measures of coverage for state transition testing. Here they are from weakest to strongest:

### *All states coverage*

To achieve 100% all states coverage the test cases must ensure that all states get visited. It is calculated like this:

```python
percent_states_coverage = visited_states / total_states * 100
```

### *Valid transitions coverage*

To achieve 100% valid transitiosn coverage the test cases must ensure that all valid transitions are exercised. This is stronger than all states coverage because all states coverage does not guarantee transition coverage, but valid transitions coverage guarantees you test all (relevant) states. Valid transitions coverage is calculated like so:

```python
percent_valid_transitions_coverage = exercised_valid_transitions / total_valid_transitions * 100
```

### *All transitions coverage*

All transitions coverage covers all transitions in a state table, including invalid transitions. You should only test one invalid transition per test case, as that prevents one invalid transition from covering the defects associated with the next (when this happens it is called ***fault masking***). All transitions coverage guarantees valid transitions coverage which guarantees all states coverage, making it the minimum requirement for important things. It is calculated with this formula:

```python
percent_all_transitions_coverage = exercised_transitions / total transitions * 100
```

Continue to *[white-box testing](/1/3/2/2.White_Box_Testing.md)*