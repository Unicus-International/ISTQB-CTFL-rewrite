# *Experience-based testing*

The name describes these techniques to a point. Here are three experience-based techniques.

## *Error guessing*

Error guessing is exactly what it sounds like. Based on knowledge of how the application has worked in the past, typical developer errors and failures in similar applications elsewhere, the tester anticipates an error and the corresponding defect and/or failure. Common sources of error are related to input, output, logic, computation, interface or data. An input might be assigned the wrong type, the logic might use the wrong operator, or something could be spelled wrong. Experience develops a good intuitive sense for likely errors that might be exposed. You test these with ***fault attacks***, methodical implementations of error guessing. This method starts with a list of plausible problems, and designs tests that will identify them in some way.

## *Exploratory testing*

Exploratory testing is *[dynamically](/1/4/1.Test_Types.md#dynamic-testing)* improvising test cases while executing them in real time. It lets you learn more about the test object, and can be helpful in test design while also potentially identifying defects. To structure exploratory testing a bit, you can organize it in time-limited sessions, followed by a debriefing discussion with concerned parties. Instead of specific test objectives, exploratory testing looks for general directions for the exploration. Throughout the sessions, you would identify what needs to be covered and exercise it, documenting the steps you follow and what you find along the way as you feel the need.
Exploratory testing is particularly useful when the specifications are inadequate or you are pressured for time, but can be a useful complement to more formal techniques. It is more effective if you are experienced and well-positioned when it comes to *[essential skills](/0/3.Essential_Skills.md)*. For example, that might allow you to incorporate other techniques, such as [equivalence partitioning](/1/4/2/1.Black_Box_Testing.md#equivalence-partitioning).

## *Checklist-based testing*

Checklist-based testing starts with a checklist. The items should be questions that can not be checked automatically, *[entry](/1/1/6.Test_Approach.md#entry-criteriadefinition-of-done)*/*[exit criteria](/1/1/6.Test_Approach.md#exit-criteriadefinition-of-done)* or overly general. Meanwhile, they *should* be possible to check independently. The items can refer to any kind of testable feature in the test object, and can be developed from experience, understanding of the user's perspective or causes of software failiure, respectively. Each item is a test formulated as a question, that can be transformed into an executable test. As a test, then, an item can become less effective over time, and new items may need to be added as new defects are found. If you are using a checklist over time you should be sure to update it regularly, adding and removing items as it becomes appropriate. Checklist-based testing is a light-weight alternative to detailed test cases which provides guidelines and some consistency. However, since they are constantly turned into tests on a case-by-case basis, they are more variable in results and hard to repeat. This also means they may end up with better coverage.

Continue to *[test execution phase](/2/0.Test-execution_Phase_Overview.md)*